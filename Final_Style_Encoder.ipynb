{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "JFD8FE4_Lzw4",
        "FsMXed_Me9TB"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Data\n"
      ],
      "metadata": {
        "id": "JFD8FE4_Lzw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_audio(audio_path, target_duration=60.0, target_sample_rate=32000):\n",
        "    try:\n",
        "        waveform, sr = librosa.load(audio_path, sr=None, mono=True)\n",
        "        if sr != target_sample_rate:\n",
        "            waveform = librosa.resample(waveform, orig_sr=sr, target_sr=target_sample_rate)\n",
        "\n",
        "        target_samples = int(target_duration * target_sample_rate)\n",
        "        if waveform.shape[0] > target_samples:\n",
        "            waveform = waveform[:target_samples]  # Truncate\n",
        "        else:\n",
        "            waveform = np.pad(waveform, (0, target_samples - waveform.shape[0]))  # Pad\n",
        "\n",
        "        return torch.tensor(waveform, dtype=torch.float16).to(device)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping {audio_path} due to error: {e}\")\n",
        "        return torch.zeros(int(target_duration * target_sample_rate), dtype=torch.float16).to(device)\n"
      ],
      "metadata": {
        "id": "pnD-Mey9T84m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pObjKlPccTN2",
        "outputId": "2bde242e-97d4-4b8b-edce-82f100a1f472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# This is just the composer and their song\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import os\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "csv_path = \"/content/drive/MyDrive/cs229Project/229Dataset/musicnet_metadata.csv\"\n",
        "data_df = pd.read_csv(csv_path, delimiter=\";\", on_bad_lines=\"skip\")\n",
        "\n",
        "audio_folder = \"/content/drive/MyDrive/cs229Project/229Dataset/musicnet/musicnet/train_data\"\n",
        "\n",
        "\n",
        "file_map = {file.split('.')[0]: os.path.join(audio_folder, file)\n",
        "             for file in os.listdir(audio_folder) if file.endswith('.wav')}\n",
        "\n",
        "data_df['audio'] = data_df['id'].astype(str).map(file_map)\n",
        "\n",
        "if 'composition' not in data_df.columns:\n",
        "    raise ValueError(\"Composer column not found in the dataset!\")\n",
        "\n",
        "updated_data_df = data_df[['id', 'composer', 'composition', 'audio']]\n",
        "\n",
        "processed_audio_paths = []\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "numpy_save_folder = \"/content/drive/MyDrive/cs229Project/229Dataset/musicnet/processed_audio/\"\n",
        "os.makedirs(numpy_save_folder, exist_ok=True)\n",
        "\n",
        "for idx, row in data_df.iterrows():\n",
        "    audio_path = row['audio']\n",
        "    if isinstance(audio_path, str) and os.path.exists(audio_path):\n",
        "        processed_waveform = load_audio(audio_path).cpu().numpy()\n",
        "        npy_path = os.path.join(numpy_save_folder, f\"{row['id']}.npy\")\n",
        "        np.save(npy_path, processed_waveform)\n",
        "        processed_audio_paths.append(npy_path)\n",
        "    else:\n",
        "        processed_audio_paths.append(None)\n",
        "\n",
        "updated_data_df['audio'] = processed_audio_paths\n",
        "\n",
        "# Save the updated CSV\n",
        "updated_csv_path = \"/content/drive/MyDrive/cs229Project/229Dataset/musicnet/meta_audiopath.csv\"\n",
        "updated_data_df.to_csv(updated_csv_path, index=False)\n",
        "\n",
        "print(f\"Updated CSV saved to: {updated_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all the composer paris\n",
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "csv_path = updated_csv_path\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "composers = df[\"composer\"].unique()\n",
        "\n",
        "composer_pairs = list(itertools.permutations(composers, 2))\n",
        "\n",
        "print(composer_pairs[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZTS8HBvfiNy",
        "outputId": "dd0c52d4-4c9c-4fbf-ea0c-d38c707371bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Schubert', 'Mozart'), ('Schubert', 'Dvorak'), ('Schubert', 'Cambini'), ('Schubert', 'Haydn'), ('Schubert', 'Brahms')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For finetune\n",
        "import os\n",
        "import pandas as pd\n",
        "import itertools\n",
        "\n",
        "# Load dataset\n",
        "csv_path = \"/content/drive/MyDrive/cs229Project/229Dataset/musicnet/meta_audiopath.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Get unique composers\n",
        "composers = df[\"composer\"].unique()\n",
        "\n",
        "composer_pairs = list(itertools.permutations(composers, 2))\n",
        "print(f\"Total composer pairs: {len(composer_pairs)}\")\n",
        "\n",
        "dataset_pairs = []\n",
        "\n",
        "for source_composer, target_composer in composer_pairs:\n",
        "    source_df = df[df[\"composer\"] == source_composer]\n",
        "    target_df = df[df[\"composer\"] == target_composer]\n",
        "\n",
        "    for _, source_row in source_df.iterrows():\n",
        "        if pd.isna(source_row[\"audio\"]) or not os.path.exists(source_row[\"audio\"]):\n",
        "            print(f\"Skipping missing source file: {source_row['audio']}\")\n",
        "            continue\n",
        "\n",
        "        target_row = target_df.sample(n=1).iloc[0]\n",
        "\n",
        "        dataset_pairs.append({\n",
        "            \"audio_path\": source_row['audio'],\n",
        "            \"text_prompt\": f\"Song Title: {source_row['composition']}. Convert this classical song to {target_composer}'s style.\"\n",
        "        })\n",
        "\n",
        "# Convert to DataFrame\n",
        "expanded_df = pd.DataFrame(dataset_pairs)\n",
        "\n",
        "# Save the dataset\n",
        "expanded_csv_path = \"/content/drive/MyDrive/cs229Project/229Dataset/musicnet/expanded_musicnet_dataset.csv\"\n",
        "expanded_df.to_csv(expanded_csv_path, index=False)\n",
        "\n",
        "print(f\"Dataset saved: {expanded_csv_path}\")\n",
        "print(expanded_df.shape)\n",
        "print(expanded_df[\"audio_path\"].iloc[0])\n",
        "print(expanded_df[\"text_prompt\"].iloc[1])\n"
      ],
      "metadata": {
        "id": "0OPeE-LYOPdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For eval\n",
        "import os\n",
        "import pandas as pd\n",
        "import itertools\n",
        "\n",
        "# Load dataset\n",
        "csv_path = \"/content/drive/MyDrive/cs229Project/229Dataset/musicnet/meta_audiopath.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "composers = df[\"composer\"].unique()\n",
        "\n",
        "composer_pairs = list(itertools.permutations(composers, 2))\n",
        "print(f\"Total composer pairs: {len(composer_pairs)}\")\n",
        "\n",
        "dataset_pairs = []\n",
        "\n",
        "for source_composer, target_composer in composer_pairs:\n",
        "    source_df = df[df[\"composer\"] == source_composer]\n",
        "    target_df = df[df[\"composer\"] == target_composer]\n",
        "\n",
        "    for _, source_row in source_df.iterrows():\n",
        "        if pd.isna(source_row[\"audio\"]) or not os.path.exists(source_row[\"audio\"]):\n",
        "            print(f\"Skipping missing source file: {source_row['audio']}\")\n",
        "            continue\n",
        "\n",
        "        target_row = target_df.sample(n=1).iloc[0]\n",
        "\n",
        "        dataset_pairs.append({\n",
        "            \"source_composer\": source_composer,\n",
        "            \"target_composer\": target_composer,\n",
        "            \"audio_path\": source_row['audio'],\n",
        "            \"text_prompt\": f\"Song Title: {source_row['composition']}. Convert this classical song to {target_composer}'s style.\"\n",
        "        })\n",
        "\n",
        "expanded_df = pd.DataFrame(dataset_pairs)\n",
        "\n",
        "expanded_csv_path = \"/content/drive/MyDrive/cs229Project/229Dataset/musicnet/expanded_musicnet_dataset_eval.csv\"\n",
        "expanded_df.to_csv(expanded_csv_path, index=False)"
      ],
      "metadata": {
        "id": "diYGPsC3htMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "PQBVHPiuMlWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Style Encoder"
      ],
      "metadata": {
        "id": "FsMXed_Me9TB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class StyleEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim=128, num_composers=10):\n",
        "        super(StyleEncoder, self).__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, kernel_size=5, stride=2, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.temporal = nn.Sequential(\n",
        "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "        self.projection = nn.Linear(256, embedding_dim)\n",
        "        self.composer_embedding = nn.Embedding(num_composers, embedding_dim)\n",
        "\n",
        "    def forward(self, x, composer_id):\n",
        "        x = self.cnn(x)\n",
        "        x = self.temporal(x).squeeze(-1)\n",
        "        composer_emb = self.composer_embedding(composer_id)\n",
        "        return self.projection(x) + composer_emb  # Style embedding\n",
        "\n",
        "def info_nce_loss(anchor, positive, negatives, temperature=0.07):\n",
        "    pos_sim = torch.cosine_similarity(anchor, positive, dim=-1)\n",
        "    neg_sim = torch.cosine_similarity(anchor.unsqueeze(1), negatives.detach(), dim=-1).mean(dim=-1)\n",
        "\n",
        "    numerator = torch.exp(pos_sim / temperature)\n",
        "    denominator = numerator + torch.sum(torch.exp(neg_sim / temperature), dim=-1)\n",
        "\n",
        "    return -torch.log(numerator / denominator).mean()\n",
        "\n",
        "import torch\n",
        "import torchaudio.transforms as T\n",
        "import librosa\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "def load_audio(audio_path, target_duration=30.0, target_sample_rate=16000):\n",
        "    try:\n",
        "        waveform, sr = librosa.load(audio_path, sr=None, mono=True)\n",
        "        if sr != target_sample_rate:\n",
        "            waveform = librosa.resample(waveform, orig_sr=sr, target_sr=target_sample_rate)\n",
        "\n",
        "        target_samples = int(target_duration * target_sample_rate)\n",
        "        if waveform.shape[0] > target_samples:\n",
        "            waveform = waveform[:target_samples]  # Truncate\n",
        "        else:\n",
        "            waveform = np.pad(waveform, (0, target_samples - waveform.shape[0]))  # Pad\n",
        "\n",
        "        return torch.tensor(waveform, dtype=torch.float32)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping {audio_path} due to error: {e}\")\n",
        "        return torch.zeros(int(target_duration * target_sample_rate), dtype=torch.float32)\n",
        "\n",
        "\n",
        "class MusicDataset(Dataset):\n",
        "    def __init__(self, df, sample_rate=32000, target_duration=30.0):\n",
        "        self.df = df.dropna(subset=['audio'])\n",
        "        self.sample_rate = sample_rate\n",
        "        self.target_duration = target_duration\n",
        "        self.composer_to_id = {composer: i for i, composer in enumerate(df['composer'].unique())}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        audio_path = row['audio_path']\n",
        "        waveform = load_audio(audio_path, target_duration=self.target_duration, target_sample_rate=self.sample_rate)\n",
        "\n",
        "        composer_id = self.composer_to_id[row['composer']]\n",
        "        return waveform.unsqueeze(0), torch.tensor(composer_id, dtype=torch.long)  # (1, T), (1,)\n",
        "\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "def train_style_encoder(model, train_loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for waveform, composer_id in train_loader:\n",
        "        waveform, composer_id = waveform.to(device), composer_id.to(device)\n",
        "\n",
        "        with autocast():\n",
        "            anchor = model(waveform, composer_id)\n",
        "            positive = model(waveform, composer_id)\n",
        "            negatives = model(waveform, composer_id)\n",
        "\n",
        "            loss = info_nce_loss(anchor, positive, negatives)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Free memory\n",
        "        del waveform, composer_id, anchor, positive, negatives\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def compute_mean_embeddings(model, dataset, device):\n",
        "    model.eval()\n",
        "    composer_embeddings = {i: [] for i in range(len(dataset.composer_to_id))}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for waveform, composer_id in dataset:\n",
        "            waveform, composer_id = waveform.to(device), composer_id.to(device)\n",
        "            embedding = model(waveform.unsqueeze(0), composer_id.unsqueeze(0))\n",
        "            composer_embeddings[composer_id.item()].append(embedding.cpu())\n",
        "\n",
        "    return {k: torch.mean(torch.stack(v), dim=0) for k, v in composer_embeddings.items()}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import pandas as pd\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    dataset = MusicDataset(data_df)\n",
        "    train_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = StyleEncoder(num_composers=len(dataset.composer_to_id)).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(10):\n",
        "        loss = train_style_encoder(model, train_loader, optimizer, device)\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), \"style_encoder.pth\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ozeH2xMSfBjx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "7d10aa56-3604-43ef-a8c2-afb8706cd600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ed8b10477c6a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_lock_unlock_module\u001b[0;34m(name)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Music Gen\n"
      ],
      "metadata": {
        "id": "nThZSOUugjKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiQqdTIkCZ-_",
        "outputId": "a4ee86e4-0fc0-493f-b75a-cf558b82c6fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "composer_map = {\n",
        "    \"Schubert\": 0,\n",
        "    \"Mozart\": 1,\n",
        "    \"Dvorak\": 2,\n",
        "    \"Cambini\": 3,\n",
        "    \"Haydn\": 4,\n",
        "    \"Brahms\": 5,\n",
        "    \"Faure\": 6,\n",
        "    \"Ravel\": 7,\n",
        "    \"Bach\": 8,\n",
        "    \"Beethoven\": 9\n",
        "}\n",
        "\n",
        "class StyleEncoder(nn.Module):\n",
        "    def __init__(self, output_dim=128, num_composers=10):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, kernel_size=5, stride=2, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.temporal = nn.LSTM(128, 128, batch_first=True)\n",
        "\n",
        "        self.projection = nn.Linear(256, output_dim)\n",
        "\n",
        "        self.composer_embedding = nn.Embedding(num_composers, output_dim)\n",
        "\n",
        "    def extract_composer_id(self, text_prompt):\n",
        "      for composer, composer_id in self.composer_map.items():\n",
        "          if f\"convert to {composer.lower()}\" in text_prompt.lower():\n",
        "              print(f\"Detected composer: {composer}, ID: {composer_id}\")\n",
        "              return torch.tensor([composer_id], dtype=torch.long, device=device)\n",
        "      return None  # No conversion requested\n",
        "\n",
        "\n",
        "    def forward(self, waveform, composer_id):\n",
        "        x = self.cnn(waveform)\n",
        "        x, _ = self.temporal(x)\n",
        "        x = self.projection(x[:, -1, :])\n",
        "\n",
        "        composer_embed = self.composer_embedding(composer_id)\n",
        "        return x + composer_embed\n",
        "\n",
        "\n",
        "style_encoder = StyleEncoder()\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/cs229Project1/style_encoder.pth\", map_location=\"cpu\")\n",
        "\n",
        "filtered_checkpoint = {k: v for k, v in checkpoint.items() if k in style_encoder.state_dict()}\n",
        "\n",
        "style_encoder.load_state_dict(filtered_checkpoint, strict=False)\n",
        "style_encoder.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_HTh0YHyTrc",
        "outputId": "d22f849d-7d4f-4af0-9f87-d4f5874ec988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StyleEncoder(\n",
              "  (cnn): Sequential(\n",
              "    (0): Conv1d(1, 32, kernel_size=(5,), stride=(2,), padding=(2,))\n",
              "    (1): ReLU()\n",
              "    (2): Conv1d(32, 64, kernel_size=(5,), stride=(2,), padding=(2,))\n",
              "    (3): ReLU()\n",
              "    (4): Conv1d(64, 128, kernel_size=(5,), stride=(2,), padding=(2,))\n",
              "    (5): ReLU()\n",
              "  )\n",
              "  (temporal): LSTM(128, 128, batch_first=True)\n",
              "  (projection): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (composer_embedding): Embedding(10, 128)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "composer_embedding_weights = style_encoder.composer_embedding.weight.data\n",
        "\n",
        "# Map composer names to their corresponding embeddings\n",
        "composer_embeddings_dict = {\n",
        "    composer: composer_embedding_weights[composer_id].detach().cpu().numpy()\n",
        "    for composer, composer_id in composer_map.items()\n",
        "}\n",
        "for composer, embedding in composer_embeddings_dict.items():\n",
        "    print(f\"{composer}: {embedding[:5]}...\")  # Print the first 5 values for brevity\n"
      ],
      "metadata": {
        "id": "fIPng-AE276f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61e2177a-37a4-484b-c9cc-f5c148f712a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schubert: [ 1.0189674  -0.03704567 -0.62848276  0.5494599   0.57478607]...\n",
            "Mozart: [-0.22611332  0.53072536 -2.9945054   1.186264   -0.8996345 ]...\n",
            "Dvorak: [-0.11956131  0.57687473 -0.14041474 -0.21242541  0.2801856 ]...\n",
            "Cambini: [-0.08229057  1.303195    0.40165764 -1.3665297  -1.1813307 ]...\n",
            "Haydn: [-0.255268    0.17314005 -0.38424298 -1.2285194   0.93682635]...\n",
            "Brahms: [ 0.06443194  0.12758182  0.17273441 -2.7166872   0.5232171 ]...\n",
            "Faure: [-1.0088236   1.2965635   0.25496018  0.25001827  0.48092383]...\n",
            "Ravel: [ 0.05876034  2.000688   -1.9079921  -1.1833687  -0.9618606 ]...\n",
            "Bach: [-0.37951747 -2.3222933   1.1933309  -0.44496727 -0.07125596]...\n",
            "Beethoven: [-0.0399006 -1.4581578  0.7540325 -0.3136256 -0.7847596]...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "from torch import nn\n",
        "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
        "import scipy.io.wavfile\n",
        "import librosa\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class EnhancedMusicGenerator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.style_encoder = StyleEncoder()  # Updated to use composer IDs\n",
        "        self.model = MusicgenForConditionalGeneration.from_pretrained(\n",
        "            \"facebook/musicgen-small\", torch_dtype=torch.float16\n",
        "        ).to(device)\n",
        "        self.processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n",
        "\n",
        "        self.composer_map = {\n",
        "            \"Schubert\": 9, \"Mozart\": 7, \"Dvorak\": 4, \"Cambini\": 3,\n",
        "            \"Haydn\": 6, \"Brahms\": 2, \"Faure\": 5, \"Ravel\": 8,\n",
        "            \"Bach\": 0, \"Beethoven\": 1\n",
        "        }\n",
        "        self.composer_mean_embeddings = self.get_composer_mean_embeddings()\n",
        "\n",
        "    def get_composer_mean_embeddings(self):\n",
        "        composer_embedding_weights = self.style_encoder.composer_embedding.weight.data\n",
        "        return {\n",
        "            composer: composer_embedding_weights[composer_id].detach().cpu().numpy()\n",
        "            for composer, composer_id in self.composer_map.items()\n",
        "        }\n",
        "\n",
        "    def extract_composer_id(self, text_prompt):\n",
        "        for composer, composer_id in self.composer_map.items():\n",
        "            if f\"Convert this classical song to {composer}'s style.\" in text_prompt.lower():\n",
        "                print(f\"Detected composer: {composer}, ID: {composer_id}\")\n",
        "                return composer, composer_id\n",
        "        return None, None\n",
        "\n",
        "    def generate_music(self, text_prompt, audio_waveform, output_path=\"generated_music.wav\", target_samples=640000):\n",
        "        composer, composer_id = self.extract_composer_id(text_prompt)\n",
        "\n",
        "        if composer is not None:\n",
        "            composer_mean_embedding = self.composer_mean_embeddings[composer]\n",
        "            composer_embedding_str = \" \".join(map(str, composer_mean_embedding))\n",
        "            text_prompt += f\" Composer Style Embedding: {composer_embedding_str}\"\n",
        "\n",
        "        if audio_waveform.shape[0] > target_samples:\n",
        "            audio_waveform = audio_waveform[:target_samples]  # Truncate\n",
        "        elif audio_waveform.shape[0] < target_samples:\n",
        "            pad_size = target_samples - audio_waveform.shape[0]\n",
        "            audio_waveform = torch.cat([audio_waveform, torch.zeros(pad_size, dtype=torch.float16, device=device)], dim=0)\n",
        "\n",
        "        audio_waveform = audio_waveform.to(dtype=torch.float16)\n",
        "\n",
        "        print(f\"Input waveform shape BEFORE MusicGen: {audio_waveform.shape}, dtype={audio_waveform.dtype}\")\n",
        "\n",
        "        input_ids = torch.tensor([ord(c) for c in text_prompt], dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_audio = self.model.generate(input_ids=input_ids, guidance_scale=3, max_new_tokens=3000)\n",
        "\n",
        "        print(f\"Generated raw audio shape: {generated_audio.shape}, dtype={generated_audio.dtype}\")\n",
        "\n",
        "        generated_audio = generated_audio.to(dtype=torch.float32).cpu().numpy()\n",
        "\n",
        "        if generated_audio.shape[-1] != target_samples:\n",
        "            if generated_audio.shape[-1] > target_samples:\n",
        "                generated_audio = generated_audio[..., :target_samples]  # Truncate\n",
        "            else:\n",
        "                pad_size = target_samples - generated_audio.shape[-1]\n",
        "                generated_audio = np.pad(generated_audio, (0, pad_size), mode='constant')\n",
        "\n",
        "        print(f\"Final processed audio shape: {generated_audio.shape}\")\n",
        "        print(f\"Generated shape: {generated_audio.shape}\")\n",
        "\n",
        "        scipy.io.wavfile.write(output_path, rate=32000, data=generated_audio)\n",
        "\n",
        "        return output_path\n"
      ],
      "metadata": {
        "id": "70cWbEQHxgba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "csv_path = \"/content/drive/MyDrive/cs229Project/229Dataset/musicnet/expanded_musicnet_dataset_eval.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Save the generated output from composer A to composer B\n",
        "output_audio_dir = \"/content/drive/MyDrive/cs229Project/229Dataset/musicnet/generated_audio\"\n",
        "os.makedirs(output_audio_dir, exist_ok=True)\n",
        "print(df[:5])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4y7ioxKRiOjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fef696d9-0e34-4345-d5a4-b8c9704305bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  source_composer target_composer  \\\n",
            "0        Schubert          Mozart   \n",
            "1        Schubert          Mozart   \n",
            "2        Schubert          Mozart   \n",
            "3        Schubert          Mozart   \n",
            "4        Schubert          Mozart   \n",
            "\n",
            "                                          audio_path  \\\n",
            "0  /content/drive/MyDrive/cs229Project/229Dataset...   \n",
            "1  /content/drive/MyDrive/cs229Project/229Dataset...   \n",
            "2  /content/drive/MyDrive/cs229Project/229Dataset...   \n",
            "3  /content/drive/MyDrive/cs229Project/229Dataset...   \n",
            "4  /content/drive/MyDrive/cs229Project/229Dataset...   \n",
            "\n",
            "                                         text_prompt  \n",
            "0  Song Title: Piano Quintet in A major. Convert ...  \n",
            "1  Song Title: Piano Quintet in A major. Convert ...  \n",
            "2  Song Title: Piano Quintet in A major. Convert ...  \n",
            "3  Song Title: Piano Quintet in A major. Convert ...  \n",
            "4  Song Title: Piano Sonata in A major. Convert t...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "75cfwg7HmwMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/enhanced_musicgen_peft.pth\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "enhanced_music_generator = EnhancedMusicGenerator()\n",
        "\n",
        "try:\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    enhanced_music_generator.load_state_dict(checkpoint)\n",
        "    print(f\"Model loaded successfully from {model_path}\")\n",
        "    enhanced_music_generator.to(device)\n",
        "    enhanced_music_generator.eval()\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Model file not found at {model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the model: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "InD81LBNqRNi",
        "outputId": "463ed065-5c5e-4c2c-ee1c-dec5253879b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'EnhancedMusicGenerator' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e8d1420bbd21>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0menhanced_music_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnhancedMusicGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'EnhancedMusicGenerator' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "music_gen = EnhancedMusicGenerator()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xholUkBJibcn",
        "outputId": "8331aff6-14aa-4b6d-aeb2-2c144201f0c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encodec/modeling_encodec.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n",
            "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
            "  \"_name_or_path\": \"t5-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 3072,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.48.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "Config of the audio_encoder: <class 'transformers.models.encodec.modeling_encodec.EncodecModel'> is overwritten by shared audio_encoder config: EncodecConfig {\n",
            "  \"_name_or_path\": \"facebook/encodec_32khz\",\n",
            "  \"architectures\": [\n",
            "    \"EncodecModel\"\n",
            "  ],\n",
            "  \"audio_channels\": 1,\n",
            "  \"chunk_length_s\": null,\n",
            "  \"codebook_dim\": 128,\n",
            "  \"codebook_size\": 2048,\n",
            "  \"compress\": 2,\n",
            "  \"dilation_growth_rate\": 2,\n",
            "  \"hidden_size\": 128,\n",
            "  \"kernel_size\": 7,\n",
            "  \"last_kernel_size\": 7,\n",
            "  \"model_type\": \"encodec\",\n",
            "  \"norm_type\": \"weight_norm\",\n",
            "  \"normalize\": false,\n",
            "  \"num_filters\": 64,\n",
            "  \"num_lstm_layers\": 2,\n",
            "  \"num_residual_layers\": 1,\n",
            "  \"overlap\": null,\n",
            "  \"pad_mode\": \"reflect\",\n",
            "  \"residual_kernel_size\": 3,\n",
            "  \"sampling_rate\": 32000,\n",
            "  \"target_bandwidths\": [\n",
            "    2.2\n",
            "  ],\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.48.3\",\n",
            "  \"trim_right_ratio\": 1.0,\n",
            "  \"upsampling_ratios\": [\n",
            "    8,\n",
            "    5,\n",
            "    4,\n",
            "    4\n",
            "  ],\n",
            "  \"use_causal_conv\": false,\n",
            "  \"use_conv_shortcut\": false\n",
            "}\n",
            "\n",
            "Config of the decoder: <class 'transformers.models.musicgen.modeling_musicgen.MusicgenForCausalLM'> is overwritten by shared decoder config: MusicgenDecoderConfig {\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"audio_channels\": 1,\n",
            "  \"bos_token_id\": 2048,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"dropout\": 0.1,\n",
            "  \"ffn_dim\": 4096,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_factor\": 0.02,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"musicgen_decoder\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_codebooks\": 4,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 2048,\n",
            "  \"scale_embedding\": false,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.48.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 2048\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Pipeline\n"
      ],
      "metadata": {
        "id": "1EO4GU3sM7-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "composer_map = {\n",
        "    \"Schubert\": 9, \"Mozart\": 7, \"Dvorak\": 4, \"Cambini\": 3,\n",
        "    \"Haydn\": 6, \"Brahms\": 2, \"Faure\": 5, \"Ravel\": 8,\n",
        "    \"Bach\": 0, \"Beethoven\": 1\n",
        "}\n",
        "reverse_composer_map = {v: k for k, v in composer_map.items()}\n"
      ],
      "metadata": {
        "id": "EiWtSx_-NT8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')\n",
        "\n",
        "def extract_yamnet_features(audio_path, sample_rate=16000):\n",
        "    audio_waveform, sr = librosa.load(audio_path, sr=sample_rate)\n",
        "\n",
        "    audio_tensor = tf.convert_to_tensor(audio_waveform, dtype=tf.float32)\n",
        "\n",
        "    if len(audio_tensor.shape) > 1:\n",
        "        audio_tensor = tf.squeeze(audio_tensor)\n",
        "\n",
        "    scores, embeddings, spectrogram = yamnet_model(audio_tensor)\n",
        "\n",
        "    return np.mean(embeddings.numpy(), axis=0)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7BTfTNxcNbEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def classify_composer(audio_features):\n",
        "    predictions = classifier_model.predict(np.expand_dims(audio_features, axis=0))\n",
        "    top_1 = np.argmax(predictions)\n",
        "    top_2 = np.argsort(predictions[0])[-2:]\n",
        "    return top_1, top_2, predictions\n",
        "\n",
        "def evaluate_musicgen():\n",
        "    df = pd.read_csv(eval_csv)\n",
        "\n",
        "    sampled_df = df.sample(n=2, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    top_1_correct = 0\n",
        "    top_2_correct = 0\n",
        "    total_samples = 0\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with tqdm(total=len(sampled_df), desc=\"Evaluating MusicGen\") as pbar:\n",
        "        for idx, row in enumerate(sampled_df.itertuples(index=False)):\n",
        "            source_composer = row.source_composer\n",
        "            target_composer = row.target_composer\n",
        "            audio_path = row.audio_path\n",
        "            text_prompt = row.text_prompt\n",
        "\n",
        "            audio_waveform = torch.tensor(np.load(audio_path, allow_pickle=True)).to(device)\n",
        "\n",
        "            generated_audio_path = music_gen.generate_music(text_prompt, audio_waveform)\n",
        "\n",
        "            audio_features = extract_yamnet_features(generated_audio_path)\n",
        "\n",
        "            top_1_pred, top_2_preds, _ = classify_composer(audio_features)\n",
        "\n",
        "            y_true.append(composer_map[target_composer])\n",
        "            y_pred.append(top_1_pred)\n",
        "\n",
        "            if top_1_pred == composer_map[target_composer]:\n",
        "                top_1_correct += 1\n",
        "\n",
        "            if composer_map[target_composer] in top_2_preds:\n",
        "                top_2_correct += 1\n",
        "\n",
        "            total_samples += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "            if (idx + 1) % 50 == 0:\n",
        "                top_1_acc = top_1_correct / total_samples\n",
        "                top_2_acc = top_2_correct / total_samples\n",
        "\n",
        "    top_1_accuracy = top_1_correct / total_samples\n",
        "    top_2_accuracy = top_2_correct / total_samples\n",
        "\n",
        "    return top_1_accuracy, top_2_accuracy\n"
      ],
      "metadata": {
        "id": "SGGIWey8NlGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_csv=\"/content/drive/MyDrive/cs229Project/229Dataset/musicnet/expanded_musicnet_dataset_eval.csv\""
      ],
      "metadata": {
        "id": "T5Hqg1D9oKQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_1, top_2 = evaluate_musicgen()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fdr4xq9DPAp3",
        "outputId": "1fc035cd-9ba5-4349-ce7d-c0bf42ff9215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating MusicGen:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input waveform shape BEFORE MusicGen: torch.Size([640000]), dtype=torch.float16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating MusicGen:   0%|          | 0/1 [04:33<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-63ba3ae4b83a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtop_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_musicgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-21e69aa0daa9>\u001b[0m in \u001b[0;36mevaluate_musicgen\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0maudio_waveform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mgenerated_audio_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmusic_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_music\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_waveform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0maudio_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_yamnet_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_audio_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-a2c30a2179a1>\u001b[0m in \u001b[0;36mgenerate_music\u001b[0;34m(self, text_prompt, audio_waveform, output_path, target_samples)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mgenerated_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguidance_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Generated raw audio shape: {generated_audio.shape}, dtype={generated_audio.dtype}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/musicgen/modeling_musicgen.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2664\u001b[0m             \u001b[0;31m# 11. run sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2665\u001b[0;31m             outputs = self._sample(\n\u001b[0m\u001b[1;32m   2666\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2667\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3255\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3256\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3257\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3259\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/musicgen/modeling_musicgen.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, input_values, padding_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2142\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   2143\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2144\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/musicgen/modeling_musicgen.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshift_tokens_right\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1278\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/musicgen/modeling_musicgen.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1183\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/musicgen/modeling_musicgen.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \u001b[0;31m# embed positions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m         \u001b[0mpositions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/musicgen/modeling_musicgen.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values_length)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LNGesghMobDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Top-1 Accuracy: {top_1}\")\n",
        "print(f\"Top-2 Accuracy: {top_2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYISbiuF82jR",
        "outputId": "d5fb4319-347e-4d13-e1a6-64fc6a66dde9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-1 Accuracy: 16.00\n",
            "Top-2 Accuracy: 23.00\n"
          ]
        }
      ]
    }
  ]
}